{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HFTk86mK60E"
      },
      "source": [
        "# 🚀 LLMOps for Production RAG\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/unionai-oss/llmops-production-rag/blob/main/workshop.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "Welcome to the LLMOps for Production RAG workshop! In this workshop, we will cover:\n",
        "\n",
        "1. Creating a baseline RAG pipeline\n",
        "2. Bootstrapping an evaluation dataset\n",
        "3. RAG Hyperparameter Optimization\n",
        "\n",
        "## 📦 Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1Ck60ZbK60J",
        "outputId": "6f736bcd-494d-4e99-9d55-11b5e7ec4150",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.2)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.7.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Cloning into 'llmops-production-rag'...\n",
            "remote: Enumerating objects: 285, done.\u001b[K\n",
            "remote: Counting objects: 100% (285/285), done.\u001b[K\n",
            "remote: Compressing objects: 100% (189/189), done.\u001b[K\n",
            "remote: Total 285 (delta 180), reused 193 (delta 92), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (285/285), 351.23 KiB | 3.38 MiB/s, done.\n",
            "Resolving deltas: 100% (180/180), done.\n",
            "/content/llmops-production-rag\n",
            "Processing /content/llmops-production-rag\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flytekit@ git+https://github.com/flyteorg/flytekit.git@bugfix/enable-notebook-registration (from llmops_rag==0.0.1)\n",
            "  Cloning https://github.com/flyteorg/flytekit.git (to revision bugfix/enable-notebook-registration) to /tmp/pip-install-xajy84nn/flytekit_ded4c5d452d242c392fad2c5a58e66ff\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/flyteorg/flytekit.git /tmp/pip-install-xajy84nn/flytekit_ded4c5d452d242c392fad2c5a58e66ff\n",
            "  Running command git checkout -b bugfix/enable-notebook-registration --track origin/bugfix/enable-notebook-registration\n",
            "  Switched to a new branch 'bugfix/enable-notebook-registration'\n",
            "  Branch 'bugfix/enable-notebook-registration' set up to track remote branch 'bugfix/enable-notebook-registration' from 'origin'.\n",
            "  Resolved https://github.com/flyteorg/flytekit.git to commit 04af7e46f32a8a1c7186a8c52b508e1cfc235059\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from llmops_rag==0.0.1) (4.13.4)\n",
            "Collecting faiss-cpu (from llmops_rag==0.0.1)\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting flashrank (from llmops_rag==0.0.1)\n",
            "  Downloading FlashRank-0.2.10-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting gradio==5.32.1 (from llmops_rag==0.0.1)\n",
            "  Downloading gradio-5.32.1-py3-none-any.whl.metadata (16 kB)\n"
          ]
        }
      ],
      "source": [
        "%pip install gradio\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    !git clone https://github.com/unionai-oss/llmops-production-rag.git\n",
        "    %cd /content/llmops-production-rag\n",
        "    !pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHZvRaSyK60L"
      },
      "source": [
        "While dependencies are being installed, create an account on Union Serverless:\n",
        "\n",
        "👉 https://signup.union.ai/\n",
        "\n",
        "Go to the Union Serverless dashboard to make sure you can check out the UI:\n",
        "\n",
        "👉 https://serverless.union.ai/\n",
        "\n",
        "Then, login to Union on this notebook session:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NjuRzg8K60M"
      },
      "outputs": [],
      "source": [
        "%cd /content/llmops-production-rag\n",
        "!union create login --auth device-flow --serverless"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A98ulb2_K60M"
      },
      "source": [
        "## 🔑 Create OpenAI API Key Secret on Union\n",
        "\n",
        "First go to https://platform.openai.com/account/api-keys and create an OpenAI API key.\n",
        "\n",
        "Then, run the following command to make the secret accessible on Union:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3B22aEJK60N"
      },
      "outputs": [],
      "source": [
        "!union create secret openai_api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feGq8nY0K60O"
      },
      "outputs": [],
      "source": [
        "!union get secret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FNzG2L9K60O"
      },
      "source": [
        "If you have issues with the secret, you can delete it by uncommenting the code cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32D3KY3tK60P"
      },
      "outputs": [],
      "source": [
        "#!union delete secret openai_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAZMcaErK60Q"
      },
      "source": [
        "## 🗂️ Creating a Baseline RAG Pipeline\n",
        "\n",
        "Create the vector store:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYb9ddT7K60Q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import union\n",
        "from typing import Optional\n",
        "from llmops_rag.vector_store import create_knowledge_base, chunk_and_embed_documents\n",
        "\n",
        "\n",
        "@union.workflow\n",
        "def create_vector_store(\n",
        "    root_url_tags_mapping: Optional[dict] = None,\n",
        "    splitter: str = \"character\",\n",
        "    chunk_size: int = 2048,\n",
        "    limit: Optional[int | float] = None,\n",
        "    embedding_model: Optional[str] = \"text-embedding-ada-002\",\n",
        "    exclude_patterns: Optional[list[str]] = None,\n",
        ") -> union.FlyteDirectory:\n",
        "    \"\"\"\n",
        "    Workflow for creating the vector store knowledge base.\n",
        "    \"\"\"\n",
        "    docs = create_knowledge_base(\n",
        "        root_url_tags_mapping=root_url_tags_mapping,\n",
        "        limit=limit,\n",
        "        exclude_patterns=exclude_patterns,\n",
        "    )\n",
        "    vector_store = chunk_and_embed_documents(\n",
        "        documents=docs,\n",
        "        splitter=splitter,\n",
        "        chunk_size=chunk_size,\n",
        "        embedding_model=embedding_model,\n",
        "    )\n",
        "    return vector_store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yczpOP2OK60R"
      },
      "source": [
        "To execute this workflow, let's create a `UnionRemote` object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrcS0AtgK60R"
      },
      "outputs": [],
      "source": [
        "remote = union.UnionRemote()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGg-xAx8K60S"
      },
      "source": [
        "Then, we execute the `create_vector_store` workflow with the `.execute()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nraetJ9SK60S"
      },
      "outputs": [],
      "source": [
        "vector_store_execution = remote.execute(\n",
        "    create_vector_store,\n",
        "    inputs=dict(\n",
        "        limit=10,\n",
        "        chunk_size=512,\n",
        "        splitter=\"character\",\n",
        "    ),\n",
        ")\n",
        "vector_store_execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKqAdxGfK60S"
      },
      "source": [
        "⚠️ Note: The above command will take a few minutes to complete since we're building our container for the first time.\n",
        "\n",
        "Then implement a basic RAG pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sTenkEqK60S"
      },
      "outputs": [],
      "source": [
        "import union\n",
        "from typing import Optional\n",
        "from llmops_rag.vector_store import VectorStore\n",
        "from llmops_rag.rag_basic import retrieve, generate\n",
        "\n",
        "\n",
        "@union.workflow\n",
        "def rag_basic(\n",
        "    questions: list[str],\n",
        "    vector_store: union.FlyteDirectory = VectorStore.query(),  # 👈 this uses the vector store artifact by default\n",
        "    embedding_model: str = \"text-embedding-ada-002\",\n",
        "    generation_model: str = \"gpt-4o-mini\",\n",
        "    search_type: str = \"similarity\",\n",
        "    rerank: bool = False,\n",
        "    num_retrieved_docs: int = 20,\n",
        "    num_docs_final: int = 5,\n",
        "    prompt_template: Optional[str] = None,\n",
        ") -> list[str]:\n",
        "    contexts = retrieve(\n",
        "        questions=questions,\n",
        "        vector_store=vector_store,\n",
        "        embedding_model=embedding_model,\n",
        "        search_type=search_type,\n",
        "        rerank=rerank,\n",
        "        num_retrieved_docs=num_retrieved_docs,\n",
        "        num_docs_final=num_docs_final,\n",
        "    )\n",
        "    return generate(\n",
        "        questions=questions,\n",
        "        contexts=contexts,\n",
        "        generation_model=generation_model,\n",
        "        prompt_template=prompt_template,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDr7T3nZK60T"
      },
      "outputs": [],
      "source": [
        "remote = union.UnionRemote()\n",
        "rag_basic_execution = remote.execute(\n",
        "    rag_basic,\n",
        "    inputs=dict(\n",
        "        questions=[\"How do I read and write a pandas dataframe to csv format?\"],\n",
        "    ),\n",
        ")\n",
        "rag_basic_execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rML_0j6KK60T"
      },
      "source": [
        "Grab the output of the rag pipeline execution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpLd4k1rK60T"
      },
      "outputs": [],
      "source": [
        "rag_basic_execution = remote.sync(remote.wait(rag_basic_execution))\n",
        "print(rag_basic_execution.outputs[\"o0\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh5SZsFfK60T"
      },
      "source": [
        "### ✨ Maintaining a Fresh Vector Store\n",
        "\n",
        "Let's use launch plan schedules to maintain a fresh vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w0b1mOHK60U"
      },
      "outputs": [],
      "source": [
        "from datetime import timedelta\n",
        "import flytekit as fl\n",
        "import union\n",
        "\n",
        "\n",
        "union.LaunchPlan.CACHE = {}\n",
        "schedule_vector_store_lp = union.LaunchPlan.get_or_create(\n",
        "    name=\"schedule_vector_store_lp\",\n",
        "    workflow=create_vector_store,\n",
        "    default_inputs=dict(\n",
        "        limit=10,\n",
        "        chunk_size=512,\n",
        "        splitter=\"character\",\n",
        "    ),\n",
        "    schedule=fl.FixedRate(\n",
        "        duration=timedelta(minutes=2)\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bm7MoKeUK60U"
      },
      "outputs": [],
      "source": [
        "version = \"workshop-v0\"\n",
        "remote = union.UnionRemote()\n",
        "\n",
        "registered_schedule_vector_store_lp = remote.register_launch_plan(\n",
        "    schedule_vector_store_lp,\n",
        "    version=version,\n",
        ")\n",
        "url = remote.generate_console_url(registered_schedule_vector_store_lp)\n",
        "remote.activate_launchplan(registered_schedule_vector_store_lp.id)\n",
        "print(f\"🚀 Launch plan activated: {url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYfQGA1OK60U"
      },
      "outputs": [],
      "source": [
        "remote.deactivate_launchplan(registered_schedule_vector_store_lp.id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4brozr87K60U"
      },
      "source": [
        "Go to the Serverless dashboard to see the schedule in action.\n",
        "\n",
        "Make sure to deactivate the launchplan in the UI!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3igk9ZVoK60U"
      },
      "source": [
        "### 💻 Run RAG pipeline with Gradio App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LD6v3u-zK60U"
      },
      "outputs": [],
      "source": [
        "import union\n",
        "import gradio as gr\n",
        "from datetime import timedelta\n",
        "\n",
        "\n",
        "def add_message(history, message):\n",
        "    if message is not None:\n",
        "        history.append({\"role\": \"user\", \"content\": message})\n",
        "    return history, gr.Textbox(value=None, interactive=False)\n",
        "\n",
        "\n",
        "def bot(history: list):\n",
        "    remote = union.UnionRemote()\n",
        "    last_user_message = [msg for msg in history if msg[\"role\"] == \"user\"][-1][\"content\"]\n",
        "    execution = remote.execute(rag_basic, inputs={\"questions\": [last_user_message]})\n",
        "    url = remote.generate_console_url(execution)\n",
        "    print(f\"🚀 Union Serverless execution url: {url}\")\n",
        "\n",
        "    answers = None\n",
        "    execution = remote.wait(execution, poll_interval=timedelta(seconds=2))\n",
        "    answers = execution.outputs[\"o0\"]\n",
        "\n",
        "    if answers is None:\n",
        "        raise RuntimeError(\"Failed to get answer\")\n",
        "\n",
        "    answer = answers[0]\n",
        "\n",
        "    history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
        "    history[-1][\"content\"] += answer\n",
        "    yield history\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(elem_id=\"chatbot\", type=\"messages\")\n",
        "\n",
        "    chat_input = gr.Textbox(\n",
        "        interactive=True,\n",
        "        placeholder=\"How do I write a dataframe to csv?\",\n",
        "        show_label=False,\n",
        "    )\n",
        "    chat_msg = chat_input.submit(\n",
        "        add_message, [chatbot, chat_input], [chatbot, chat_input]\n",
        "    )\n",
        "    bot_msg = chat_msg.then(bot, chatbot, chatbot, api_name=\"bot_response\")\n",
        "    bot_msg.then(lambda: gr.Textbox(interactive=True), None, [chat_input])\n",
        "\n",
        "\n",
        "demo.launch(debug=True, share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAjTOeRcK60V"
      },
      "source": [
        "## 🥾 Bootstrapping an Evaluation Dataset\n",
        "\n",
        "Then generate a question and answer dataset. This will use the raw knowledge base we created\n",
        "in the previous step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4ziORnuK60V"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "from typing import Annotated\n",
        "\n",
        "import union\n",
        "from llmops_rag.create_qa_dataset import generate_qa_datapoints, create_dataset, QuestionAndAnswerDataset\n",
        "from llmops_rag.document import CustomDocument\n",
        "from llmops_rag.vector_store import KnowledgeBase\n",
        "\n",
        "\n",
        "@union.workflow\n",
        "def create_qa_dataset(\n",
        "    documents: list[CustomDocument] = KnowledgeBase.query(),\n",
        "    n_questions_per_doc: int = 1,\n",
        "    n_answers_per_question: int = 5,\n",
        ") -> Annotated[union.FlyteFile, QuestionAndAnswerDataset]:\n",
        "    partial_task = functools.partial(\n",
        "        generate_qa_datapoints,\n",
        "        n_questions_per_doc=n_questions_per_doc,\n",
        "        n_answers_per_question=n_answers_per_question,\n",
        "    )\n",
        "    questions_and_answers = union.map_task(partial_task)(flyte_doc=documents)\n",
        "    return create_dataset(questions_and_answers, n_answers_per_question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aP0-L4DtK60V"
      },
      "outputs": [],
      "source": [
        "remote = union.UnionRemote()\n",
        "qa_dataset_execution = remote.execute(\n",
        "    create_qa_dataset,\n",
        "    inputs={\"n_questions_per_doc\": 3, \"n_answers_per_question\": 5}\n",
        ")\n",
        "qa_dataset_execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-EV2PgwK60V"
      },
      "source": [
        "Filter the dataset with an LLM critic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5Lqx3-RK60V"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import union\n",
        "from llmops_rag.create_llm_filtered_dataset import apply_llm_critic, filter_dataset, prepare_dataset\n",
        "from llmops_rag.create_qa_dataset import QuestionAndAnswerDataset\n",
        "\n",
        "\n",
        "@union.workflow\n",
        "def create_llm_filtered_dataset(\n",
        "    dataset: union.FlyteFile = QuestionAndAnswerDataset.query(),\n",
        ") -> pd.DataFrame:\n",
        "    scores = apply_llm_critic(dataset)\n",
        "    reference_answers = filter_dataset(dataset, scores)\n",
        "    return prepare_dataset(reference_answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REUAPkfjK60W"
      },
      "outputs": [],
      "source": [
        "remote = union.UnionRemote()\n",
        "filtered_dataset_execution = remote.execute(create_llm_filtered_dataset, inputs={})\n",
        "filtered_dataset_execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01ihgLxbK60W"
      },
      "source": [
        "## 📊 RAG Hyperparameter Optimization\n",
        "\n",
        "Experiment with different embedding models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmUGhy90K60W"
      },
      "outputs": [],
      "source": [
        "import union\n",
        "from typing import Optional, Annotated\n",
        "from llmops_rag.optimize_rag import (\n",
        "    GridSearchConfig,\n",
        "    prepare_hpo_configs,\n",
        "    prepare_questions,\n",
        "    gridsearch,\n",
        "    combine_answers,\n",
        "    evaluate,\n",
        "    report,\n",
        ")\n",
        "from llmops_rag.config import RAGConfig\n",
        "from llmops_rag.create_llm_filtered_dataset import EvalDatasetArtifact\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "@union.workflow\n",
        "def optimize_rag(\n",
        "    gridsearch_config: GridSearchConfig,\n",
        "    root_url_tags_mapping: Optional[dict] = None,\n",
        "    exclude_patterns: Optional[list[str]] = None,\n",
        "    limit: Optional[int] = 10,\n",
        "    eval_dataset: Annotated[pd.DataFrame, EvalDatasetArtifact] = EvalDatasetArtifact.query(dataset_type=\"llm_filtered\"),\n",
        "    eval_prompt_template: Optional[str] = None,\n",
        "    n_answers: int = 1,\n",
        ") -> RAGConfig:\n",
        "    hpo_configs = prepare_hpo_configs(gridsearch_config)\n",
        "    questions = prepare_questions(eval_dataset, n_answers)\n",
        "    answers = gridsearch(questions, hpo_configs, root_url_tags_mapping, exclude_patterns, limit)\n",
        "    answers_dataset = combine_answers(answers, hpo_configs, questions)\n",
        "    best_config, evaluation, evalution_summary = evaluate(\n",
        "        answers_dataset, eval_prompt_template\n",
        "    )\n",
        "    report(evaluation, evalution_summary)\n",
        "    return best_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPy51WEGK60W"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "def run_experiment(config_path: str):\n",
        "    with open(config_path, \"r\") as f:\n",
        "        gridsearch_config = GridSearchConfig(**yaml.safe_load(f))\n",
        "\n",
        "    remote = union.UnionRemote()\n",
        "    execution = remote.execute(optimize_rag, inputs={\"gridsearch_config\": gridsearch_config})\n",
        "    return execution\n",
        "\n",
        "\n",
        "run_experiment(\"config/embedding_model_experiment.yaml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZqVmnLDK60W"
      },
      "source": [
        "### 🧪 More experiments to run (optional)\n",
        "\n",
        "Uncomment the code cells below to run different experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzF_16niK60W"
      },
      "source": [
        "Experiment with different prompts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2dpq17dK60W"
      },
      "outputs": [],
      "source": [
        "# run_experiment(\"config/prompt_experiment.yaml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hJuL5m_K60b"
      },
      "source": [
        "Experiment with different chunksizes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkPCnW6OK60b"
      },
      "outputs": [],
      "source": [
        "# run_experiment(\"config/chunksize_experiment.yaml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myFIu7lZK60b"
      },
      "source": [
        "Experiment with different splitters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rw9umXlKK60b"
      },
      "outputs": [],
      "source": [
        "# run_experiment(\"config/splitter_experiment.yaml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjjflWReK60c"
      },
      "source": [
        "Experiment with reranking:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZfxFz4sK60c"
      },
      "outputs": [],
      "source": [
        "# run_experiment(\"config/reranking_experiment.yaml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PPYpC90K60c"
      },
      "source": [
        "Experiment with document retrieval:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4YWabJuK60c"
      },
      "outputs": [],
      "source": [
        "# run_experiment(\"config/search_params_experiment.yaml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5kn_ionK60c"
      },
      "source": [
        "## 🔄 Putting it all together into a reactive pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cogWdQqJK60d"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Annotated\n",
        "from datetime import timedelta\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import union\n",
        "import flytekit as fl\n",
        "from union.artifacts import OnArtifact\n",
        "\n",
        "from llmops_rag.document import CustomDocument\n",
        "from llmops_rag.create_qa_dataset import create_qa_dataset\n",
        "from llmops_rag.create_llm_filtered_dataset import create_llm_filtered_dataset, EvalDatasetArtifact\n",
        "from llmops_rag.image import image\n",
        "from llmops_rag.optimize_rag import optimize_rag, GridSearchConfig\n",
        "from llmops_rag.vector_store import create_knowledge_base as _create_knowledge_base\n",
        "\n",
        "\n",
        "# Clear the launch plan cache\n",
        "union.LaunchPlan.CACHE = {}\n",
        "\n",
        "\n",
        "KnowledgeBaseHPO = union.Artifact(name=\"knowledge-base-hpo\")\n",
        "\n",
        "\n",
        "@union.task(\n",
        "    container_image=image,\n",
        "    requests=union.Resources(cpu=\"2\", mem=\"8Gi\"),\n",
        "    enable_deck=True,\n",
        ")\n",
        "def create_knowledge_base_hpo(\n",
        "    root_url_tags_mapping: Optional[dict] = None,\n",
        "    limit: Optional[int | float] = None,\n",
        "    exclude_patterns: Optional[list[str]] = None,\n",
        ") -> Annotated[list[CustomDocument], KnowledgeBaseHPO]:\n",
        "    return _create_knowledge_base.task_function(\n",
        "        root_url_tags_mapping=root_url_tags_mapping,\n",
        "        limit=limit,\n",
        "        exclude_patterns=exclude_patterns,\n",
        "    )\n",
        "\n",
        "\n",
        "@union.workflow\n",
        "def knowledge_base_workflow(\n",
        "    root_url_tags_mapping: Optional[dict] = None,\n",
        "    limit: Optional[int | float] = None,\n",
        "    exclude_patterns: Optional[list[str]] = None,\n",
        ") -> list[CustomDocument]:\n",
        "    return create_knowledge_base_hpo(\n",
        "        root_url_tags_mapping=root_url_tags_mapping,\n",
        "        limit=limit,\n",
        "        exclude_patterns=exclude_patterns,\n",
        "    )\n",
        "\n",
        "\n",
        "@union.workflow\n",
        "def create_eval_dataset(\n",
        "    documents: list[CustomDocument],\n",
        "    n_questions_per_doc: int = 1,\n",
        "    n_answers_per_question: int = 5,\n",
        ") -> pd.DataFrame:\n",
        "    qa_dataset = create_qa_dataset(\n",
        "        documents=documents,\n",
        "        n_questions_per_doc=n_questions_per_doc,\n",
        "        n_answers_per_question=n_answers_per_question,\n",
        "    )\n",
        "    return create_llm_filtered_dataset(dataset=qa_dataset)\n",
        "\n",
        "\n",
        "knowledge_base_lp = union.LaunchPlan.get_or_create(\n",
        "    knowledge_base_workflow,\n",
        "    name=\"knowledge_base_lp\",\n",
        "    default_inputs={\"limit\": 10},\n",
        "    schedule=fl.FixedRate(duration=timedelta(minutes=2))\n",
        ")\n",
        "\n",
        "create_eval_dataset_lp = union.LaunchPlan.get_or_create(\n",
        "    create_eval_dataset,\n",
        "    name=\"create_eval_dataset_lp\",\n",
        "    trigger=OnArtifact(\n",
        "        trigger_on=KnowledgeBaseHPO,\n",
        "        inputs={\"documents\": KnowledgeBaseHPO.query()},\n",
        "    )\n",
        ")\n",
        "\n",
        "optimize_rag_lp = union.LaunchPlan.get_or_create(\n",
        "    optimize_rag,\n",
        "    name=\"optimize_rag_lp\",\n",
        "    default_inputs={\n",
        "        \"gridsearch_config\": GridSearchConfig(\n",
        "            embedding_model=[\n",
        "                \"text-embedding-ada-002\",\n",
        "                \"text-embedding-3-small\",\n",
        "                \"text-embedding-3-large\",\n",
        "            ],\n",
        "            chunk_size=[256],\n",
        "            splitter=[\"recursive\"],\n",
        "        ),\n",
        "    },\n",
        "    trigger=OnArtifact(\n",
        "        trigger_on=EvalDatasetArtifact,\n",
        "        inputs={\"eval_dataset\": EvalDatasetArtifact.query()},\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKjLcIadK60d"
      },
      "outputs": [],
      "source": [
        "remote = union.UnionRemote()\n",
        "\n",
        "version = \"workshop-v0\"\n",
        "registered_lps = []\n",
        "for lp in [\n",
        "    knowledge_base_lp,\n",
        "    create_eval_dataset_lp,\n",
        "    optimize_rag_lp,\n",
        "]:\n",
        "    registered_lp = remote.register_launch_plan(lp, version=version)\n",
        "    registered_lps.append(registered_lp)\n",
        "    remote.activate_launchplan(registered_lp.id)\n",
        "    url = remote.generate_console_url(registered_lp)\n",
        "    print(f\"🚀 Launch plan {lp.name} activated: {url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej7TGka8K60d"
      },
      "source": [
        "Deactivate the reactive pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7u72w_5dK60d"
      },
      "outputs": [],
      "source": [
        "for registered_lp in registered_lps:\n",
        "    remote.deactivate_launchplan(registered_lp.id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz-XiyxgK60d"
      },
      "source": [
        "## 🎉 Congrats!\n",
        "\n",
        "You've completed the LLMOps for Production RAG workshop! To recap, you:\n",
        "- Built a simple baseline RAG pipeline\n",
        "- Scheduled a job to maintain a fresh vector store\n",
        "- Bootstrapped an evaluation dataset\n",
        "- Optimized the RAG pipeline with HPO and LLM-as-a-judge"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}